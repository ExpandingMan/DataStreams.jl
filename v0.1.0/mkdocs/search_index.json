{
    "docs": [
        {
            "location": "/", 
            "text": "DataStreams.jl\n\n\nThe \nDataStreams.jl\n package aims to define a generic and performant framework for the transfer of \"table-like\" data. (i.e. data that can, at least in some sense, be described by rows and columns).\n\n\nThe framework achieves this by defining interfaces (i.e. a group of methods) for \nData.Source\n types and methods to describe how they \"provide\" data; as well as \nData.Sink\n types and methods around how they \"receive\" data. This allows \nData.Source\ns and \nData.Sink\ns to implement their interfaces separately, without needing to be aware of each other. The end result is an ecosystem of packages that \"automatically\" talk with each other, with adding an additional package not requiring changes to existing packages.\n\n\nPackages can have a single julia type implement both the \nData.Source\n and \nData.Sink\n interfaces, or two separate types can implement them separately. For examples of interface implementations, see some of the packages below:\n\n\nData.Source\n implementations:\n\n\n\n\nCSV.Source\n\n\nSQLite.Source\n\n\nDataFrame\n\n\nODBC.Source\n\n\n\n\nData.Sink\n implementations:\n\n\n\n\nCSV.Sink\n\n\nSQLite.Sink\n\n\nDataFrame\n\n\nODBC.Sink\n\n\n\n\n\n\nData.Source\n Interface\n\n\nThe \nData.Source\n interface requires the following definitions, where \nMyPkg\n would represent a package wishing to implement the interface:\n\n\n\n\nData.schema(::MyPkg.Source) =\n Data.Schema\n; get the \nData.Schema\n of a \nData.Source\n. Typically the \nSource\n type will store the \nData.Schema\n directly, but this isn't strictly required. See \n?Data.Schema\n or docs below for more information on \nData.Schema\n\n\nData.isdone(::MyPkg.Source, row, col) =\n Bool\n; indicates whether the \nData.Source\n will be able to provide a value at a given a \nrow\n and \ncol\n.\n\n\n\n\nOptional definition:\n\n\n\n\nData.reference(::MyPkg.Source) =\n Vector{UInt8}\n; Sometimes, a \nSource\n needs the \nSink\n to keep a reference to memory to keep a data structure valid. A \nSource\n can implement this method to return a \nVector{UInt8}\n that the \nSink\n will need to handle appropriately.\n\n\nBase.size(::MyPkg.Source[, i]) =\n Int\n; not explicitly required to enable data-streaming, but a \nSource\n should generally be able to describe its first 2 dimensions, i.e. # of rows and columns.\n\n\n\n\nA \nData.Source\n also needs to \"register\" the type (or types) of streaming it supports. Currently defined streaming types in the DataStreams framework include:\n\n\n\n\nData.Field\n: a field is the intersection of a specific row and column; this type of streaming will traverse the \"table\" structure by row, accessing each column on each row\n\n\nData.Column\n: this type of streaming will provide entire columns at a time\n\n\n\n\nA \nData.Source\n formally supports \nfield-based\n streaming by defining the following:\n\n\n\n\nData.streamtype(::Type{MyPkg.Source}, ::Type{Data.Field}) = true\n; declares that \nMyPkg.Source\n supports field-based streaming\n\n\nData.streamfrom{T}(::MyPkg.Source, ::Type{Data.Field}, ::Type{Nullable{T}}, row, col) =\n Nullable{T}\n; returns a value of type \nNullable{T}\n given a specific \nrow\n and \ncol\n from \nMyPkg.Source\n\n\nData.streamfrom{T}(::MyPkg.Source, ::Type{Data.Field}, ::Type{T}, row, col) =\n T\n; returns a value of type \nT\n given a specific \nrow\n and \ncol\n from \nMyPkg.Source\n\n\n\n\nAnd for column-based streaming:\n\n\n\n\nData.streamtype(::Type{MyPkg.Source}, ::Type{Data.Column}) = true\n\n\nData.streamfrom{T}(::Data.Source, ::Type{Data.Column}, ::Type{T}, col) =\n Vector{T}\n; Given a type \nT\n, returns column # \ncol\n of a \nData.Source\n as a \nVector{T}\n\n\nData.streamfrom{T}(::Data.Source, ::Type{Data.Column}, ::Type{Nullable{T}}, col) =\n NullableVector{T}\n; Given a type \nNullable{T}\n, returns column # \ncol\n of a \nData.Source\n as a \nNullableVector{T}\n\n\n\n\n\n\nData.Sink\n Interface\n\n\nSimilar to a \nData.Source\n, a \nData.Sink\n needs to \"register\" the types of streaming it supports, it does so through the following definition:\n\n\n\n\nData.streamtypes(::Type{MyPkg.Sink}) = [Data.Field[, Data.Column]]\n; \"registers\" the streaming preferences for \nMyPkg.Sink\n. A \nSink\n type should list the stream type or types it supports. If the \nSink\n supports streaming of multiple types, it should list them in order of preference (i.e. the more natural or performant type first).\n\n\n\n\nA \nData.Sink\n needs to also implement specific forms of constructors that ensure proper Sink state in many higher-level streaming functions:\n\n\n\n\nMyPkg.Sink{T \n: Data.StreamType}(schema::Data.Schema, ::Type{T}, append::Bool, ref::Vector{UInt8}, args...; kwargs...)\n; given a \nschema::Data.Schema\n a source will be providing, the type of streaming \nT\n (\nField\n or \nColumn\n), whether the user desires to append the data or not, a possible memory reference \nref\n and any necessary \nargs...\n and \nkwargs...\n, construct an appropriate instance of \nMyPkg.Sink\n ready to receive data from \nsource\n. The \nappend\n argument allows an already existing sink file/source to \"reset\" itself if the user does not desire to append.\n\n\nMyPkg.Sink{T \n: Data.StreamType}(sink, schema::Data.Schema, ::Type{T}, append::Bool, ref::Vector{UInt8})\n; similar to above, but instead of constructing a new \nSink\n, an existing \nSink\n is given as a first argument, which may be modified before being returned, ready to receive data according to the \nData.Source\n \nschema\n.\n\n\n\n\nSimilar to \nData.Source\n, a \nData.Sink\n also needs to implement it's own \nstreamto!\n method that indicates how it receives data.\n\n\nA \nData.Sink\n supports \nfield-based\n streaming by defining:\n\n\n\n\nData.streamto!{T}(sink::MyPkg.Sink, ::Type{Data.Field}, val::T, row, col[, schema])\n: Given a \nrow\n, \ncol\n, and \nval::T\n a \nData.Sink\n should store the value appropriately. The type of the value retrieved is given by \nT\n, which may be \nNullable{T}\n. Optionally provided is the \nschema\n (the same \nschema\n that is passed in the \nMyPkg.Sink(schema, ...)\n constructors). This argument is passed for efficiency since\n\n\n\n\nit can be calculated once at the beginning of a \nData.stream!\n and used quickly for many calls to \nData.streamto!\n. This argument is optional, because a Sink can overload \nData.streamto!\n with or without it.\n\n\nA \nData.Sink\n supports \ncolumn-based\n streaming by defining:\n\n\n* `Data.streamto!{T}(sink::MyPkg.Sink, ::Type{Data.Column}, column::Type{T}, row, col[, schema])`: Given a column number `col` and column of data `column`, a `Data.Sink` should store it appropriately. The type of the column is given by `T`, which may be a `NullableVector{T}`. Optionally provided is the `schema` (the same `schema` that is passed in the `MyPkg.Sink(schema, ...)` constructors). This argument is passed for efficiency since it can be calculated once at the beginning of a `Data.stream!` and used quickly for many calls to `Data.streamto!`. This argument is optional, because a Sink can overload `Data.streamto!` with or without it.\n\n\n\n\nA \nData.Sink\n can optionally define the following if needed:\n\n\n\n\nData.cleanup!(sink::MyPkg.Sink)\n: certain \nData.Sink\n, like databases, may need to protect against inconvenient or dangerous \"states\" if there happens to be an error while streaming. \nData.cleanup!\n provides the sink a way to rollback a transaction or other kind of cleanup if an error occurs during streaming\n\n\nData.close!(sink::MyPkg.Sink)\n: during the \nData.stream!\n workflow, a \nData.Sink\n should remain \"open\" to receiving data until \nData.close!\n is call explicitly. \nData.close!\n is defined to allow a sink to fully commit all streaming results and close/destroy any necessary resources.\n\n\n\n\n\n\nData.Schema\n\n\n#\n\n\nDataStreams.Data.Schema\n \n \nType\n.\n\n\nA \nData.Schema\n describes a tabular dataset (i.e. a set of optionally named, typed columns with records as rows)\n\n\nData.Schema\n allow \nData.Source\n and \nData.Sink\n to talk to each other and prepare to provide/receive data through streaming. \nData.Schema\n fields include:\n\n\n\n\nA boolean type parameter that indicates whether the # of rows is known in the \nData.Source\n; this is useful as a type parameter to allow \nData.Sink\n and \nData.streamto!\n methods to dispatch. Note that the sentinel value \n-1\n is used as the # of rows when the # of rows is unknown.\n\n\nData.header(schema)\n to return the header/column names in a \nData.Schema\n\n\nData.types(schema)\n to return the column types in a \nData.Schema\n; \nNullable{T}\n indicates columns that may contain missing data (null values)\n\n\nData.size(schema)\n to return the (# of rows, # of columns) in a \nData.Schema\n\n\n\n\nData.Source\n and \nData.Sink\n interfaces both require that \nData.schema(source_or_sink)\n be defined to ensure that other \nData.Source\n/\nData.Sink\n can work appropriately.\n\n\nsource", 
            "title": "Home"
        }, 
        {
            "location": "/#datastreamsjl", 
            "text": "The  DataStreams.jl  package aims to define a generic and performant framework for the transfer of \"table-like\" data. (i.e. data that can, at least in some sense, be described by rows and columns).  The framework achieves this by defining interfaces (i.e. a group of methods) for  Data.Source  types and methods to describe how they \"provide\" data; as well as  Data.Sink  types and methods around how they \"receive\" data. This allows  Data.Source s and  Data.Sink s to implement their interfaces separately, without needing to be aware of each other. The end result is an ecosystem of packages that \"automatically\" talk with each other, with adding an additional package not requiring changes to existing packages.  Packages can have a single julia type implement both the  Data.Source  and  Data.Sink  interfaces, or two separate types can implement them separately. For examples of interface implementations, see some of the packages below:  Data.Source  implementations:   CSV.Source  SQLite.Source  DataFrame  ODBC.Source   Data.Sink  implementations:   CSV.Sink  SQLite.Sink  DataFrame  ODBC.Sink", 
            "title": "DataStreams.jl"
        }, 
        {
            "location": "/#datasource-interface", 
            "text": "The  Data.Source  interface requires the following definitions, where  MyPkg  would represent a package wishing to implement the interface:   Data.schema(::MyPkg.Source) =  Data.Schema ; get the  Data.Schema  of a  Data.Source . Typically the  Source  type will store the  Data.Schema  directly, but this isn't strictly required. See  ?Data.Schema  or docs below for more information on  Data.Schema  Data.isdone(::MyPkg.Source, row, col) =  Bool ; indicates whether the  Data.Source  will be able to provide a value at a given a  row  and  col .   Optional definition:   Data.reference(::MyPkg.Source) =  Vector{UInt8} ; Sometimes, a  Source  needs the  Sink  to keep a reference to memory to keep a data structure valid. A  Source  can implement this method to return a  Vector{UInt8}  that the  Sink  will need to handle appropriately.  Base.size(::MyPkg.Source[, i]) =  Int ; not explicitly required to enable data-streaming, but a  Source  should generally be able to describe its first 2 dimensions, i.e. # of rows and columns.   A  Data.Source  also needs to \"register\" the type (or types) of streaming it supports. Currently defined streaming types in the DataStreams framework include:   Data.Field : a field is the intersection of a specific row and column; this type of streaming will traverse the \"table\" structure by row, accessing each column on each row  Data.Column : this type of streaming will provide entire columns at a time   A  Data.Source  formally supports  field-based  streaming by defining the following:   Data.streamtype(::Type{MyPkg.Source}, ::Type{Data.Field}) = true ; declares that  MyPkg.Source  supports field-based streaming  Data.streamfrom{T}(::MyPkg.Source, ::Type{Data.Field}, ::Type{Nullable{T}}, row, col) =  Nullable{T} ; returns a value of type  Nullable{T}  given a specific  row  and  col  from  MyPkg.Source  Data.streamfrom{T}(::MyPkg.Source, ::Type{Data.Field}, ::Type{T}, row, col) =  T ; returns a value of type  T  given a specific  row  and  col  from  MyPkg.Source   And for column-based streaming:   Data.streamtype(::Type{MyPkg.Source}, ::Type{Data.Column}) = true  Data.streamfrom{T}(::Data.Source, ::Type{Data.Column}, ::Type{T}, col) =  Vector{T} ; Given a type  T , returns column #  col  of a  Data.Source  as a  Vector{T}  Data.streamfrom{T}(::Data.Source, ::Type{Data.Column}, ::Type{Nullable{T}}, col) =  NullableVector{T} ; Given a type  Nullable{T} , returns column #  col  of a  Data.Source  as a  NullableVector{T}", 
            "title": "Data.Source Interface"
        }, 
        {
            "location": "/#datasink-interface", 
            "text": "Similar to a  Data.Source , a  Data.Sink  needs to \"register\" the types of streaming it supports, it does so through the following definition:   Data.streamtypes(::Type{MyPkg.Sink}) = [Data.Field[, Data.Column]] ; \"registers\" the streaming preferences for  MyPkg.Sink . A  Sink  type should list the stream type or types it supports. If the  Sink  supports streaming of multiple types, it should list them in order of preference (i.e. the more natural or performant type first).   A  Data.Sink  needs to also implement specific forms of constructors that ensure proper Sink state in many higher-level streaming functions:   MyPkg.Sink{T  : Data.StreamType}(schema::Data.Schema, ::Type{T}, append::Bool, ref::Vector{UInt8}, args...; kwargs...) ; given a  schema::Data.Schema  a source will be providing, the type of streaming  T  ( Field  or  Column ), whether the user desires to append the data or not, a possible memory reference  ref  and any necessary  args...  and  kwargs... , construct an appropriate instance of  MyPkg.Sink  ready to receive data from  source . The  append  argument allows an already existing sink file/source to \"reset\" itself if the user does not desire to append.  MyPkg.Sink{T  : Data.StreamType}(sink, schema::Data.Schema, ::Type{T}, append::Bool, ref::Vector{UInt8}) ; similar to above, but instead of constructing a new  Sink , an existing  Sink  is given as a first argument, which may be modified before being returned, ready to receive data according to the  Data.Source   schema .   Similar to  Data.Source , a  Data.Sink  also needs to implement it's own  streamto!  method that indicates how it receives data.  A  Data.Sink  supports  field-based  streaming by defining:   Data.streamto!{T}(sink::MyPkg.Sink, ::Type{Data.Field}, val::T, row, col[, schema]) : Given a  row ,  col , and  val::T  a  Data.Sink  should store the value appropriately. The type of the value retrieved is given by  T , which may be  Nullable{T} . Optionally provided is the  schema  (the same  schema  that is passed in the  MyPkg.Sink(schema, ...)  constructors). This argument is passed for efficiency since   it can be calculated once at the beginning of a  Data.stream!  and used quickly for many calls to  Data.streamto! . This argument is optional, because a Sink can overload  Data.streamto!  with or without it.  A  Data.Sink  supports  column-based  streaming by defining:  * `Data.streamto!{T}(sink::MyPkg.Sink, ::Type{Data.Column}, column::Type{T}, row, col[, schema])`: Given a column number `col` and column of data `column`, a `Data.Sink` should store it appropriately. The type of the column is given by `T`, which may be a `NullableVector{T}`. Optionally provided is the `schema` (the same `schema` that is passed in the `MyPkg.Sink(schema, ...)` constructors). This argument is passed for efficiency since it can be calculated once at the beginning of a `Data.stream!` and used quickly for many calls to `Data.streamto!`. This argument is optional, because a Sink can overload `Data.streamto!` with or without it.  A  Data.Sink  can optionally define the following if needed:   Data.cleanup!(sink::MyPkg.Sink) : certain  Data.Sink , like databases, may need to protect against inconvenient or dangerous \"states\" if there happens to be an error while streaming.  Data.cleanup!  provides the sink a way to rollback a transaction or other kind of cleanup if an error occurs during streaming  Data.close!(sink::MyPkg.Sink) : during the  Data.stream!  workflow, a  Data.Sink  should remain \"open\" to receiving data until  Data.close!  is call explicitly.  Data.close!  is defined to allow a sink to fully commit all streaming results and close/destroy any necessary resources.", 
            "title": "Data.Sink Interface"
        }, 
        {
            "location": "/#dataschema", 
            "text": "#  DataStreams.Data.Schema     Type .  A  Data.Schema  describes a tabular dataset (i.e. a set of optionally named, typed columns with records as rows)  Data.Schema  allow  Data.Source  and  Data.Sink  to talk to each other and prepare to provide/receive data through streaming.  Data.Schema  fields include:   A boolean type parameter that indicates whether the # of rows is known in the  Data.Source ; this is useful as a type parameter to allow  Data.Sink  and  Data.streamto!  methods to dispatch. Note that the sentinel value  -1  is used as the # of rows when the # of rows is unknown.  Data.header(schema)  to return the header/column names in a  Data.Schema  Data.types(schema)  to return the column types in a  Data.Schema ;  Nullable{T}  indicates columns that may contain missing data (null values)  Data.size(schema)  to return the (# of rows, # of columns) in a  Data.Schema   Data.Source  and  Data.Sink  interfaces both require that  Data.schema(source_or_sink)  be defined to ensure that other  Data.Source / Data.Sink  can work appropriately.  source", 
            "title": "Data.Schema"
        }
    ]
}